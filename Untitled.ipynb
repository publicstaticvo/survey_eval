{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f13696da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_local(fn):\n",
    "    with open(fn, \"r+\", encoding=\"utf-8\") as f:\n",
    "        d = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "    return d\n",
    "\n",
    "def print_json(d, fn):\n",
    "    with open(fn, \"w+\", encoding=\"utf-8\") as f:\n",
    "        for x in d:\n",
    "            f.write(json.dumps(x, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def yield_local(fn):\n",
    "    with open(fn, \"r+\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                try:\n",
    "                    yield json.loads(line)\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9265cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M.2160\n",
      "[LatexCharsNode(parsing_state=<parsing state 2310916137072>, pos=0, len=6, chars='M.2160')]\n",
      "M.2160\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"P:\\\\AI4S\\\\survey_eval\\\\latex_parser\\\\\")\n",
    "from latex_parser.tex_parser import LatexPaperParser, process_input_commands\n",
    "# base = \"P:\\\\AI4S\\\\survey_eval\\\\crawled_papers\\\\cs\\\\2501.00842\"\n",
    "# with open(f\"{base}\\\\elsarticle-template-num.tex\", encoding='utf-8') as f:\n",
    "#     content = process_input_commands(f.read(), base)\n",
    "# parser = LatexPaperParser(content, base)\n",
    "# paper = parser.parse()\n",
    "# all_citations = paper.map_citations_to_sentence()\n",
    "# print(len(all_citations))\n",
    "from pylatexenc.latexwalker import LatexWalker\n",
    "from pylatexenc.latex2text import LatexNodes2Text\n",
    "content = \"\\\\href{https://www.itu.int/rec/R-REC-M.2160/en}{M.2160}\\\\href{https://www.itu.int/rec/R-REC-M.2160/en}\"\n",
    "import re\n",
    "\n",
    "print(content)\n",
    "from pylatexenc.latex2text import LatexNodes2Text\n",
    "\n",
    "nodes, _, _ = LatexWalker(content).get_latex_nodes()\n",
    "converter = LatexNodes2Text(\n",
    "    math_mode=\"verbatim\",\n",
    ")\n",
    "print(nodes)\n",
    "print(converter.nodelist_to_text(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a83e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibtexparser\n",
    "\n",
    "def parse_bib_file(filepath: str):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        bib_database = bibtexparser.load(f)\n",
    "    citations = {}\n",
    "    for entry in bib_database.entries:\n",
    "        citation_key = entry.get('ID', '')\n",
    "        if citation_key and citation_key not in citations:\n",
    "            citations[citation_key] = entry\n",
    "    return citations\n",
    "\n",
    "parse_bib_file(\"P:\\\\AI4S\\\\survey_eval\\\\crawled_papers\\\\cs\\\\2502.15573\\\\anthology.bib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d4b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "arxiv_pattern = re.compile(r\"(?<![0-9])[0-9]{4}\\.[0-9]{4,5}(?![0-9])\")\n",
    "\n",
    "def get_arxiv_cites(f):\n",
    "    arxiv_index, no_arxiv_titles = set(), set()\n",
    "    citations = load_local(f)\n",
    "    for x in citations:\n",
    "        cite = x['citation']\n",
    "        for k in cite:\n",
    "            arxiv_key = \"\"\n",
    "            if 'journal' in cite[k]:\n",
    "                arxiv_key = arxiv_pattern.findall(cite[k]['journal'])\n",
    "                if arxiv_key: arxiv_key = arxiv_key[-1]\n",
    "            if not arxiv_key and 'volume' in cite[k]:\n",
    "                arxiv_key = arxiv_pattern.findall(cite[k]['volume'])\n",
    "                if arxiv_key: arxiv_key = arxiv_key[-1]\n",
    "            if arxiv_key:\n",
    "                cite[k]['journal'] = 'arXiv'\n",
    "                cite[k]['volume'] = arxiv_key\n",
    "                arxiv_index.add(arxiv_key)\n",
    "            else:\n",
    "                no_arxiv_titles.add(cite[k]['title'])\n",
    "    return citations, arxiv_index, no_arxiv_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3047af87-c71d-45dc-a82d-9a8bc0aaad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import time\n",
    "\n",
    "def search_arxiv_by_title(title, max_results=3, retry=3):\n",
    "    max_retry = retry\n",
    "    while retry > 0:\n",
    "        try:\n",
    "            time.sleep(3 ** (max_retry - retry))\n",
    "            client = arxiv.Client()\n",
    "            search = arxiv.Search(query=f'ti:\"{title}\"', max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\n",
    "            results = list(client.results(search))\n",
    "            return results[0].entry_id.split(\"/\")[-1] if results else None\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}, Retry: {retry}\")\n",
    "            retry -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a4b103f3-2a01-4375-856a-71a5d0de2b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 2, 'offset': 0, 'data': [{'paperId': '048d133e2ec513ce385c8e736df715d8ff496e17', 'url': 'https://www.semanticscholar.org/paper/048d133e2ec513ce385c8e736df715d8ff496e17', 'title': 'Recipe1M: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images', 'citationCount': 344, 'openAccessPdf': {'url': '', 'status': None, 'license': None}}, {'paperId': '02c009f41b66d2f977fb663f3cb69329f0f03d3f', 'url': 'https://www.semanticscholar.org/paper/02c009f41b66d2f977fb663f3cb69329f0f03d3f', 'title': 'Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images', 'citationCount': 141, 'openAccessPdf': {'url': 'https://dspace.mit.edu/bitstream/1721.1/130340/2/tpami19.pdf', 'status': 'GREEN', 'license': 'CCBYNCSA'}}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "while True:\n",
    "    url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    title = \"Recipe1m+: A dataset for learning cross-modal embeddings for cooking recipes and food images\"\n",
    "    params = {'query': title, 'limit': 3, 'fields': 'paperId,title,openAccessPdf,url,citationCount'}\n",
    "    # with self.session.with_api_key(self.api_key[self.times_429 % len(self.api_key)]) as temp_session:                \n",
    "    response = requests.get(url, params=params, timeout=60)\n",
    "    if response.status_code == 200: break\n",
    "    time.sleep(1)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fa7725ee-57f8-4bee-b37d-8e2c3a5c4b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 24/724 [00:17<05:45,  2.02it/s]Bozo feed; consider handling: document declared as utf-8, but parsed as MacRoman\n",
      "100%|██████████| 724/724 [07:56<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inline arXiv': 62130, 'arXiv': 1460, 's2': 2471, 'Network Error': 29, '': 45660}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm, glob\n",
    "arxiv_pattern = re.compile(r\"(?<![0-9])[0-9]{4}\\.[0-9]{4,5}(?![0-9])\")\n",
    "paper_dataset = {\"inline arXiv\": [], \"arXiv\": [], \"s2\": [], \"Network Error\": [], \"\": []}\n",
    "arxiv_set, arxiv_set_7 = set(), {}\n",
    "\n",
    "def fetch_arxiv_id_7(title):\n",
    "    title = f'ti:{cite['title']}'\n",
    "    client = arxiv.Client(num_retries=5)\n",
    "    search = arxiv.Search(query=title, max_results=3, sort_by=arxiv.SortCriterion.Relevance)\n",
    "    for result in client.results(search):\n",
    "        text = result.entry_id.split(\"/\")\n",
    "        return f\"{text[-2]}/{text[-1]}\"\n",
    "\n",
    "\n",
    "for f in tqdm.tqdm(glob.glob(\"crawled_papers/cs/*/citations-clean.jsonl\")):\n",
    "    for i, x in enumerate(yield_local(f)):\n",
    "        for m in x['citation']:\n",
    "            cite = x['citation'][m]\n",
    "            cite['paper'] = f.split(\"/\")[-2]\n",
    "            cite['sentence'] = i\n",
    "            cite['key'] = m\n",
    "            if \"source\" in cite: \n",
    "                if \"arXiv\" in cite['source']:\n",
    "                    if len(cite['volume']) == 7:\n",
    "                        if cite['volume'] in arxiv_set_7: cite['volume'] = arxiv_set_7[cite['volume']]\n",
    "                        else: \n",
    "                            arxiv_id = fetch_arxiv_id_7(cite['title'])\n",
    "                            if arxiv_id: cite['volume'] = arxiv_set_7[cite['volume']] = arxiv_id\n",
    "                            else: continue\n",
    "                    else:\n",
    "                        arxiv_set.add(cite['volume'])\n",
    "                paper_dataset[cite['source']].append(cite)\n",
    "            else:\n",
    "                if \"journal\" in cite:\n",
    "                    arxiv_id = arxiv_pattern.findall(cite['journal'])\n",
    "                    if arxiv_id:\n",
    "                        cite['source'] = \"inline arXiv\"\n",
    "                        cite['volume'] = arxiv_id[0]\n",
    "                        paper_dataset[cite['source']].append(cite)\n",
    "                        continue\n",
    "                    arxiv_id = re.findall(r\"(?<![0-9])[0-9]{7}(?![0-9])\", cite['journal'])\n",
    "                    if arxiv_id:\n",
    "                        cite['source'] = \"inline arXiv\"\n",
    "                        if arxiv_id[0] in arxiv_set_7: cite['volume'] = arxiv_set_7[arxiv_id[0]]\n",
    "                        else:\n",
    "                            arxiv_id = fetch_arxiv_id_7(cite['title'])\n",
    "                            if arxiv_id: cite['volume'] = arxiv_set_7[cite['volume']] = arxiv_id\n",
    "                            else: continue\n",
    "                        paper_dataset[cite['source']].append(cite)\n",
    "                        continue\n",
    "                if \"volume\" in cite:\n",
    "                    arxiv_id = arxiv_pattern.findall(cite['volume'])\n",
    "                    if arxiv_id:\n",
    "                        cite['source'] = \"inline arXiv\"\n",
    "                        cite['volume'] = arxiv_id[0]\n",
    "                        paper_dataset[cite['source']].append(cite)\n",
    "                        continue\n",
    "                    arxiv_id = re.findall(r\"(?<![0-9])[0-9]{7}(?![0-9])\", cite['volume'])\n",
    "                    if arxiv_id:\n",
    "                        cite['source'] = \"inline arXiv\"\n",
    "                        if arxiv_id[0] in arxiv_set_7: cite['volume'] = arxiv_set_7[arxiv_id[0]]\n",
    "                        else:\n",
    "                            arxiv_id = fetch_arxiv_id_7(cite['title'])\n",
    "                            if arxiv_id: cite['volume'] = arxiv_set_7[cite['volume']] = arxiv_id\n",
    "                            else: continue\n",
    "                        paper_dataset[cite['source']].append(cite)\n",
    "                        continue\n",
    "                paper_dataset[''].append(cite)\n",
    "print({k: len(v) for k, v in paper_dataset.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1d226e44-318e-460d-9db7-14391436a5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inline arXiv': 49199, 'arXiv': 813, 's2': 4592, 'Network Error': 18, '': 10048, 'no title': 174}\n"
     ]
    }
   ],
   "source": [
    "paper_dataset_f = {\"inline arXiv\": [], \"arXiv\": [], \"s2\": [], \"Network Error\": [], \"\": [], \"no title\": []}\n",
    "title_to_url = {}\n",
    "for p in paper_dataset:\n",
    "    for x in paper_dataset[p]:\n",
    "        if 'title' not in x: \n",
    "            if 'info' not in x: \n",
    "                paper_dataset_f['no title'].append(x)\n",
    "                continue\n",
    "            else: x['title'] = x['info']\n",
    "        if x['title'] not in title_to_url:\n",
    "            if 'arXiv' in p:\n",
    "                title_to_url[x['title']] = x['volume']\n",
    "            elif p == 's2':\n",
    "                title_to_url[x['title']] = x['url']\n",
    "            elif 'url' in x:\n",
    "                x['source'] = 's2'\n",
    "                title_to_url[x['title']] = x['url']\n",
    "                paper_dataset_f['s2'].append(x)\n",
    "                continue\n",
    "            else:\n",
    "                if 'source' in x: del x['source']\n",
    "            paper_dataset_f[p].append(x)\n",
    "print({k: len(v) for k, v in paper_dataset_f.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "32a45ec7-57b6-41f7-be19-06ee30ad7984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'year': '2021', 'pages': '187--203', 'number': '1', 'volume': '43', 'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'author': 'Mar{\\\\i}n, Javier and Biswas, Aritro and Ofli, Ferda and Hynes, Nicholas and Salvador, Amaia and Aytar, Yusuf and Weber, Ingmar and Torralba, Antonio', 'title': 'Recipe1m+: A dataset for learning cross-modal embeddings for cooking recipes and food images', 'ENTRYTYPE': 'article', 'ID': 'marin2021recipe1m+', 'paper': '2501.01958', 'sentence': 9, 'key': 'marin2021recipe1m+'}\n"
     ]
    }
   ],
   "source": [
    "print(paper_dataset_f[''][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "304240df-9ae6-46f2-b3ea-d1cc70009577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inline arXiv': 49199, 'arXiv': 813, 's2': 4592, 'Network Error': 18, '': 10048, 'no title': 174}\n"
     ]
    }
   ],
   "source": [
    "# print_json([x for p in paper_dataset for x in paper_dataset[p]], \"crawled_papers/citations/all.jsonl\")\n",
    "print({k: len(v) for k, v in paper_dataset_f.items()})\n",
    "paper_dataset_f[''] += paper_dataset_f['Network Error']\n",
    "del paper_dataset_f['Network Error']\n",
    "for p in paper_dataset_f:\n",
    "    print_json(paper_dataset_f[p], f'crawled_papers/citations/{p.replace(\" \", \"_\") if p else \"null\"}.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae70f7d-1bd1-42eb-8217-c179130d82d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm, glob\n",
    "arxiv_set = {}\n",
    "for f in tqdm.tqdm(glob.glob(\"crawled_papers/cs/*/citations-clean.jsonl\")):\n",
    "    d = load_local(f)\n",
    "    for x in d:\n",
    "        for m in x['citation']:\n",
    "            cite = x['citation'][m]\n",
    "            if \"source\" in cite and \"arXiv\" in cite['source'] and len(cite['volume']) == 7:\n",
    "                if cite['volume'] in arxiv_set:\n",
    "                    cite['volume'] = arxiv_set[cite['volume']]\n",
    "                    continue\n",
    "                while True:\n",
    "                    try:\n",
    "                        title = f'ti:{cite['title']}'\n",
    "                        client = arxiv.Client(num_retries=retry)\n",
    "                        search = arxiv.Search(query=title, max_results=3, sort_by=arxiv.SortCriterion.Relevance)\n",
    "                        for r in client.results(search):\n",
    "                            text = result.entry_id.split(\"/\")\n",
    "                            cite['volume'] = arxiv_set[cite['volume']] = f\"{text[-2]}/{text[-1]}\"\n",
    "                            break\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        pass\n",
    "    print_json(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61e4601e-5017-49a0-8c35-818a566d66ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-21\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\"https://api.semanticscholar.org/datasets/v1/release/latest\").json()\n",
    "print(response['release_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bf1595ad-7fbd-41e4-80d9-909cd1152788",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"paper_to_query/seeds.json\") as f:\n",
    "    papers = json.load(f)\n",
    "data = [{\"id\": k, \"title\": v['title'], \"abstract\": v['abstract']} for k, v in papers.items()]\n",
    "print_json(data, \"../agenteval/inputs/seeds.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "306ccd24-cbde-4bc5-aa35-d5175860d14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50025\n"
     ]
    }
   ],
   "source": [
    "with open(\"paper_to_query/easy_neg.json\") as f:\n",
    "    hard = json.load(f)\n",
    "d = [{\n",
    "    \"positive\": {\n",
    "        \"id\": v['related_id'], \n",
    "        \"title\": papers[v['related_id']]['title'], \n",
    "        \"abstract\": papers[v['related_id']]['abstract']\n",
    "    }, \n",
    "    \"negative\": {\n",
    "        \"id\": k, \n",
    "        \"title\": v['title'], \n",
    "        \"abstract\": v['abstract']\n",
    "    }\n",
    "} for k, v in hard.items()]\n",
    "print(len(d))\n",
    "print_json(d, \"../agenteval/inputs/easy_neg.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c5addf41-4d9a-45b8-b384-6244ecbebd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54604 42248 3115 0\n"
     ]
    }
   ],
   "source": [
    "with open(\"crawled_papers/citations/urls.json\") as f:\n",
    "    url_map = json.load(f)\n",
    "import glob\n",
    "pdf_files = [x.replace(\"crawled_papers/pdf/\", \"\").replace(\".pdf\", \"\") for x in glob.glob(\"crawled_papers/pdf/*.pdf\")]\n",
    "urls = set()\n",
    "for title, v in url_map.items():\n",
    "    if \"arxiv.org\" in v: title = v.replace(\"https://arxiv.org/pdf/\", \"\")\n",
    "    title = title.replace(\" \", \"+\").replace(\":\", \"--\")\n",
    "    urls.add(title)\n",
    "diffa = urls - set(pdf_files)\n",
    "diffb = set(pdf_files) - urls\n",
    "print(len(url_map), len(set(pdf_files)), len(diffa), len(diffb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bc3da46c-5f2d-491a-a264-c1dd3ecbc574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inline_arXiv': 49199} 49199\n"
     ]
    }
   ],
   "source": [
    "sources = ['inline_arXiv']\n",
    "paper_dataset = {k: load_local(f\"crawled_papers/citations/{k}.jsonl\") for k in sources}\n",
    "urls_map = {}\n",
    "for k in paper_dataset:\n",
    "    for x in paper_dataset[k]:\n",
    "        # urls_map[x['title']] = x['url'] if k == \"s2\" else f\"https://arxiv.org/pdf/{x['volume']}\"\n",
    "        urls_map[x['title']] = f\"https://arxiv.org/pdf/{x['volume']}\"\n",
    "print({k: len(v) for k, v in paper_dataset.items()}, len(urls_map))\n",
    "with open(\"crawled_papers/citations/urls_inlinearxiv.json\", \"w+\") as f: json.dump(urls_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "96274601-74e3-4f92-8f74-881076aee759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inline_arXiv': 49199} 49025 174\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "urls_map_exists, urls_map_missing = {}, {}\n",
    "for k in paper_dataset:\n",
    "    for x in paper_dataset[k]:\n",
    "        title = x['title']\n",
    "        url = x['url'] if k == \"s2\" else f\"https://arxiv.org/pdf/{x['volume']}\"\n",
    "        # urls_map[x['title']] = f\"https://arxiv.org/pdf/{x['volume']}\"\n",
    "        path = url.replace(\"https://arxiv.org/pdf/\", \"\") if \"arxiv.org\" in url else title\n",
    "        path = path.replace(\" \", \"+\").replace(\":\", \"--\")\n",
    "        if os.path.exists(f'crawled_papers/pdf/{path}.pdf'): urls_map_exists[title] = path\n",
    "        else: urls_map_missing[title] = url\n",
    "print({k: len(v) for k, v in paper_dataset.items()}, len(urls_map_exists), len(urls_map_missing))\n",
    "with open(\"crawled_papers/citations/urls_inlinearXiv.json\", \"w+\") as f: json.dump(urls_map_exists, f)\n",
    "with open(\"crawled_papers/citations/inlinearXiv_redownload.json\", \"w+\") as f: json.dump(urls_map_missing, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "dec45dfb-8578-4fe1-b695-45ac43b26bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n"
     ]
    }
   ],
   "source": [
    "# with open(\"crawled_papers/citations/find.json\") as f: \n",
    "#     urls_map_missing.update(json.load(f))\n",
    "null = load_local(\"crawled_papers/citations/null.jsonl\")\n",
    "os_source = {}\n",
    "for k in ['s2', 'arXiv']:\n",
    "    for x in yield_local(f\"crawled_papers/citations/{k}.jsonl\"):\n",
    "        os_source[x['title']] = x['url'] if k == \"s2\" else f\"https://arxiv.org/pdf/{x['volume']}\"\n",
    "old_urls = {}\n",
    "for x in null:\n",
    "    if x in os_source:\n",
    "        url = os_source[x].replace(\"arxiv.org/abs/\", \"arxiv.org/pdf/\").replace(\"openreview.net/forum\", \"openreview.net/pdf\")\n",
    "        if \"pdf\" not in url: url += \".pdf\"\n",
    "        old_urls[x] = url\n",
    "print(len(old_urls))\n",
    "with open(\"crawled_papers/citations/old_urls.json\", \"w+\") as f: json.dump(old_urls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "aa4581e3-452d-4bdd-bd0c-e0668dc0982a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5006 43011 40813 48654 160 4653 2346 48615 2008\n"
     ]
    }
   ],
   "source": [
    "# 一共四个文件：\n",
    "# urls_inlinearXiv.json: 初次下载成功的inline arxiv。 -- urls_map_exists\n",
    "# with open(\"crawled_papers/citations/urls_inlinearXiv.json\") as f: urls_map_exists = json.load(f)\n",
    "# inlinearXiv_redownload.json: 下载失败的inline arxiv。 -- urls_map_missing\n",
    "# with open(\"crawled_papers/citations/inlinearXiv_redownload.json\") as f: urls_map_missing = json.load(f)\n",
    "# find.json: s2api请求到的文献以及请求失败的文献中，用openalex再次找到的路径。 -- find\n",
    "# with open(\"crawled_papers/citations/find.json\") as f: find = json.load(f)\n",
    "# old_urls.json: openalex请求失败了但曾经被s2api请求成功的文献。 -- old_urls\n",
    "# with open(\"crawled_papers/citations/old_urls.json\") as f: old_urls = json.load(f)\n",
    "\n",
    "def construct_title(title, url):\n",
    "    if \"arxiv.org\" in url: title = url.replace(\"https://arxiv.org/pdf/\", \"\")\n",
    "    return title.replace(\" \", \"+\").replace(\":\", \"--\").split(\"/\")[-1].replace(\"{\", \"\").replace(\"}\", \"\").replace(\".pdf\", \"\")\n",
    "    \n",
    "pdf_files = set(x.split(\"/\")[-1].replace(\"{\", \"\").replace(\"}\", \"\").replace(\".pdf\", \"\") for x in os.listdir(\"crawled_papers/pdf\"))\n",
    "parse_success = set(x[:-4] for x in os.listdir(\"crawled_papers/paper_info\"))\n",
    "inline_titles = set(construct_title(k, v) for k, v in urls_map_exists.items())\n",
    "redownload_titles = set(construct_title(k, v) for k, v in urls_map_missing.items())\n",
    "find_titles = set(construct_title(k, v) for k, v in find.items())\n",
    "old_titles = set(construct_title(k, v) for k, v in old_urls.items())\n",
    "broken_pdfs = inline_titles - parse_success\n",
    "new_pdfs = pdf_files - parse_success - broken_pdfs\n",
    "print(len(find), len(pdf_files), len(parse_success), len(inline_titles), len(redownload_titles), \n",
    "      len(find_titles), len(old_titles), len(broken_pdfs), len(new_pdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "bb603cae-ae63-4cb3-ac6c-a745db34c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in glob.glob(\"crawled_papers/pdf/*.pdf\"):\n",
    "    filename = f.split(\"/\")[-1]\n",
    "    newf = re.sub(r\"[\\[\\]\\{\\}\\(\\)\\n]\", \"\", filename)\n",
    "    newf = \"/\".join(f.split(\"/\")[:-1] + [newf])\n",
    "    if f != newf: \n",
    "        os.rename(f, newf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9ee32542-d5a5-4390-8110-53ea9e6c06e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40813\n"
     ]
    }
   ],
   "source": [
    "pd = os.listdir(\"crawled_papers/paper_info\")\n",
    "print(len(pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e250f034-606e-4465-9b4b-1da8a82bdeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43011 41930 1081 616\n"
     ]
    }
   ],
   "source": [
    "def normalize_title(title: str, url: str) -> tuple[str, str]:\n",
    "    if \"https://arxiv.org/pdf/\" in url: title = url.replace(\"https://arxiv.org/pdf/\", \"\")\n",
    "    elif \"https://arxiv.org/abs/\" in url: \n",
    "        title = url.replace(\"https://arxiv.org/abs/\", \"\")\n",
    "        url = url.replace(\"abs\", \"pdf\")\n",
    "    if \"/\" in title: title = title.split(\"/\")[-1]\n",
    "    title = title.replace(\" \", \"+\").replace(\":\", \"--\")\n",
    "    title = re.sub(r\"[\\{\\}\\[\\]\\(\\)\\n]\", \"\", title)\n",
    "    return title, url\n",
    "    \n",
    "pdfs = set(x[:-4] for x in os.listdir(\"crawled_papers/pdf\"))\n",
    "xmls = set(x[:-4] for x in os.listdir(\"crawled_papers/papers_full\"))\n",
    "diffs = pdfs - xmls\n",
    "redownload = {}\n",
    "for d in [urls_map_exists, urls_map_missing, find, old_urls]:\n",
    "    for k, v in d.items():\n",
    "        if len(d) == 49025 and \"http\" not in v:\n",
    "            v = f\"https://arxiv.org/pdf/{v}\"\n",
    "        title, url = normalize_title(k, v)\n",
    "        if title in diffs: redownload[title] = url\n",
    "print(len(pdfs), len(xmls), len(diffs), len(redownload))\n",
    "with open(\"crawled_papers/citations/redownload.json\", \"w+\") as f: json.dump(redownload, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ad043a13-b334-4f6f-ba08-e51a5fdc6f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "609\n"
     ]
    }
   ],
   "source": [
    "print(len(set(redownload.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6915b466-93d3-4709-a217-5f0be13ec411",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = load_local(\"agent/final.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "35f0e5c0-15da-4c76-b4b0-6e6dfb61c63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4237 5\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "all_examples, no_positive = [], []\n",
    "for x in final:\n",
    "    positive = [y['features'] for y in x['citations'] if y['query'] is not None]\n",
    "    if not positive: \n",
    "        no_positive.append(x['query'])\n",
    "        continue\n",
    "    positive_ids = set(y['id'] for y in x['citations'] if y['query'] is not None)\n",
    "    #  and y['features'][3] > 0.001 and y['features'][0] > 0.4\n",
    "    negative = [y['features'] for i, y in x['oracle'].items() if i not in positive_ids]\n",
    "    if len(negative) > 8 * len(positive):\n",
    "        negative = random.sample(negative, 8 * len(positive))\n",
    "    for x in positive: x[3] *= 100\n",
    "    for x in negative: x[3] *= 100\n",
    "    all_examples.append({\"chosen\": positive, \"rejected\": negative})\n",
    "print(len(all_examples), len(no_positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35447948-fd41-4aea-b0d1-e2e9cc92e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_json(all_examples, \"train_unfiltered.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6be1b9e9-59d6-49ac-9db9-ba409462ad84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5802272093214645, 0.6951921335227317, 0.9200738986424688, 0.005647254731297929, 0.2634244151202588]\n",
      "[0.4782271881188792, 0.6697714662520363, 0.5970057169885848, 0.0014639512639257279, 0.2793163962788106]\n",
      "[0.5282662710935353, 0.5075860226891465, 0.5848955292849121, 0.0008251858097541986, 0.19789319473796205]\n",
      "[0.2820138724558999, 0.39636830908366794, 0.0, 0.0003648024571642535, 0.2462774773860048]\n",
      "[0.5820143310951718, 0.44224436735918976, 0.2433336164139801, 0.0004170158729369283, 0.2272686473082467]\n",
      "[0.37203137140012726, 0.42418842747308594, 0.3650004246209701, 0.0004627312004300431, 0.20399151357263628]\n",
      "[0.4879600489543501, 0.42928524072714397, 0.4361709450044923, 0.0006402247470968661, 0.1727570895927895]\n",
      "[0.39342385826511284, 0.5602162994458415, 0.341561912870932, 0.0005187760248957936, 0.21227894262058866]\n",
      "[0.5090515687739146, 0.5373698828284396, 0.0, 0.0003648024571642535, 0.1896679909247869]\n",
      "[0.4468559640265191, 0.7091222426616733, 0.45022068946860055, 0.0008631726986542585, 0.3079678769828234]\n",
      "\n",
      "[0.34403801487785723, 0.775604098344059, 0.5848955292849121, 0.001238605805465904, 0.33684058005237183]\n",
      "[0.5803346003209584, 0.5808051976859898, 0.6720341531270154, 0.0011265966031641752, 0.24221458054262046]\n",
      "[0.6244827739675003, 0.5231007599944666, 0.5970057169885848, 0.0009048762164515988, 0.22717977354499413]\n",
      "[0.7097956602973452, 0.3141205283658589, 0.3145041367975023, 0.0004496133486803297, 0.19517405793603165]\n",
      "[0.6795920860365707, 0.5594710933349865, 0.5258351966050626, 0.0007416572020801384, 0.2333175679862271]\n",
      "[0.6164854848296929, 0.44049861224370573, 0.5343992414614442, 0.0006097623657907389, 0.21183505446748824]\n",
      "[0.5555908612144362, 0.5754871496856858, 0.5785119857715367, 0.0007835006601047546, 0.27675081898228787]\n",
      "[0.5115794639734749, 0.4900523449815844, 0.3650004246209701, 0.00047405166351901715, 0.30448664169991707]\n",
      "[0.6129198221070002, 0.5311808089728887, 0.4973085575596037, 0.0005700753310452229, 0.23068889422983083]\n",
      "[0.4158323796827742, 0.46717631197902587, 0.2825015801910825, 0.00042804853996409513, 0.29027296323128887]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10): print(n4[i])\n",
    "print()\n",
    "for i in range(10): print(oracles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bba1d89d-85d5-43b0-bb63-95834c69dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in final:\n",
    "    del x['hard_negatives'], x['easy_negatives']\n",
    "print_json(final, \"agent/final.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622a6b36-c439-4a6e-bd41-570a3634fadf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d0ab4ff-a00b-4525-8de6-34f43bbebb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'W2920708523', 'title': 'Prevalence, Risk Factors, and Fetomaternal Outcomes of Gestational Diabetes Mellitus in Kuwait: A Cross-Sectional Study', 'features': [0.5308709851132949, 0.29138664379250934, 0.0, 0, 0.21019103299036604], 'query': None}\n"
     ]
    }
   ],
   "source": [
    "for y in x['citations']:\n",
    "    if not y['query']:\n",
    "        print(y)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bfaf7803-342b-4578-acd3-c03975173325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effects of Nutritional Strategies on Glucose Homeostasis in Gestational Diabetes Mellitus: A Systematic Review and Network Meta-Analysis\n"
     ]
    }
   ],
   "source": [
    "print(x['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03907537-b9b1-476e-80da-ed2cf509830d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
