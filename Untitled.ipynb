{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f13696da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_local(fn):\n",
    "    with open(fn, \"r+\", encoding=\"utf-8\") as f:\n",
    "        d = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "    return d\n",
    "\n",
    "def print_json(d, fn):\n",
    "    with open(fn, \"w+\", encoding=\"utf-8\") as f:\n",
    "        for x in d:\n",
    "            f.write(json.dumps(x, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def yield_local(fn):\n",
    "    with open(fn, \"r+\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                try:\n",
    "                    yield json.loads(line)\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9265cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M.2160\n",
      "[LatexCharsNode(parsing_state=<parsing state 2310916137072>, pos=0, len=6, chars='M.2160')]\n",
      "M.2160\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"P:\\\\AI4S\\\\survey_eval\\\\latex_parser\\\\\")\n",
    "from latex_parser.tex_parser import LatexPaperParser, process_input_commands\n",
    "# base = \"P:\\\\AI4S\\\\survey_eval\\\\crawled_papers\\\\cs\\\\2501.00842\"\n",
    "# with open(f\"{base}\\\\elsarticle-template-num.tex\", encoding='utf-8') as f:\n",
    "#     content = process_input_commands(f.read(), base)\n",
    "# parser = LatexPaperParser(content, base)\n",
    "# paper = parser.parse()\n",
    "# all_citations = paper.map_citations_to_sentence()\n",
    "# print(len(all_citations))\n",
    "from pylatexenc.latexwalker import LatexWalker\n",
    "from pylatexenc.latex2text import LatexNodes2Text\n",
    "content = \"\\\\href{https://www.itu.int/rec/R-REC-M.2160/en}{M.2160}\\\\href{https://www.itu.int/rec/R-REC-M.2160/en}\"\n",
    "import re\n",
    "\n",
    "print(content)\n",
    "from pylatexenc.latex2text import LatexNodes2Text\n",
    "\n",
    "nodes, _, _ = LatexWalker(content).get_latex_nodes()\n",
    "converter = LatexNodes2Text(\n",
    "    math_mode=\"verbatim\",\n",
    ")\n",
    "print(nodes)\n",
    "print(converter.nodelist_to_text(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a83e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibtexparser\n",
    "\n",
    "def parse_bib_file(filepath: str):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        bib_database = bibtexparser.load(f)\n",
    "    citations = {}\n",
    "    for entry in bib_database.entries:\n",
    "        citation_key = entry.get('ID', '')\n",
    "        if citation_key and citation_key not in citations:\n",
    "            citations[citation_key] = entry\n",
    "    return citations\n",
    "\n",
    "parse_bib_file(\"P:\\\\AI4S\\\\survey_eval\\\\crawled_papers\\\\cs\\\\2502.15573\\\\anthology.bib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d4b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "arxiv_pattern = re.compile(r\"(?<![0-9])[0-9]{4}\\.[0-9]{4,5}(?![0-9])\")\n",
    "\n",
    "def get_arxiv_cites(f):\n",
    "    arxiv_index, no_arxiv_titles = set(), set()\n",
    "    citations = load_local(f)\n",
    "    for x in citations:\n",
    "        cite = x['citation']\n",
    "        for k in cite:\n",
    "            arxiv_key = \"\"\n",
    "            if 'journal' in cite[k]:\n",
    "                arxiv_key = arxiv_pattern.findall(cite[k]['journal'])\n",
    "                if arxiv_key: arxiv_key = arxiv_key[-1]\n",
    "            if not arxiv_key and 'volume' in cite[k]:\n",
    "                arxiv_key = arxiv_pattern.findall(cite[k]['volume'])\n",
    "                if arxiv_key: arxiv_key = arxiv_key[-1]\n",
    "            if arxiv_key:\n",
    "                cite[k]['journal'] = 'arXiv'\n",
    "                cite[k]['volume'] = arxiv_key\n",
    "                arxiv_index.add(arxiv_key)\n",
    "            else:\n",
    "                no_arxiv_titles.add(cite[k]['title'])\n",
    "    return citations, arxiv_index, no_arxiv_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3047af87-c71d-45dc-a82d-9a8bc0aaad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import time\n",
    "\n",
    "def search_arxiv_by_title(title, max_results=3, retry=3):\n",
    "    max_retry = retry\n",
    "    while retry > 0:\n",
    "        try:\n",
    "            time.sleep(3 ** (max_retry - retry))\n",
    "            client = arxiv.Client()\n",
    "            search = arxiv.Search(query=f'ti:\"{title}\"', max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\n",
    "            results = list(client.results(search))\n",
    "            return results[0].entry_id.split(\"/\")[-1] if results else None\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}, Retry: {retry}\")\n",
    "            retry -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a4b103f3-2a01-4375-856a-71a5d0de2b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 2, 'offset': 0, 'data': [{'paperId': '048d133e2ec513ce385c8e736df715d8ff496e17', 'url': 'https://www.semanticscholar.org/paper/048d133e2ec513ce385c8e736df715d8ff496e17', 'title': 'Recipe1M: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images', 'citationCount': 344, 'openAccessPdf': {'url': '', 'status': None, 'license': None}}, {'paperId': '02c009f41b66d2f977fb663f3cb69329f0f03d3f', 'url': 'https://www.semanticscholar.org/paper/02c009f41b66d2f977fb663f3cb69329f0f03d3f', 'title': 'Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images', 'citationCount': 141, 'openAccessPdf': {'url': 'https://dspace.mit.edu/bitstream/1721.1/130340/2/tpami19.pdf', 'status': 'GREEN', 'license': 'CCBYNCSA'}}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "while True:\n",
    "    url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    title = \"Recipe1m+: A dataset for learning cross-modal embeddings for cooking recipes and food images\"\n",
    "    params = {'query': title, 'limit': 3, 'fields': 'paperId,title,openAccessPdf,url,citationCount'}\n",
    "    # with self.session.with_api_key(self.api_key[self.times_429 % len(self.api_key)]) as temp_session:                \n",
    "    response = requests.get(url, params=params, timeout=60)\n",
    "    if response.status_code == 200: break\n",
    "    time.sleep(1)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fa7725ee-57f8-4bee-b37d-8e2c3a5c4b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 24/724 [00:17<05:45,  2.02it/s]Bozo feed; consider handling: document declared as utf-8, but parsed as MacRoman\n",
      "100%|██████████| 724/724 [07:56<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inline arXiv': 62130, 'arXiv': 1460, 's2': 2471, 'Network Error': 29, '': 45660}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm, glob\n",
    "arxiv_pattern = re.compile(r\"(?<![0-9])[0-9]{4}\\.[0-9]{4,5}(?![0-9])\")\n",
    "paper_dataset = {\"inline arXiv\": [], \"arXiv\": [], \"s2\": [], \"Network Error\": [], \"\": []}\n",
    "arxiv_set, arxiv_set_7 = set(), {}\n",
    "\n",
    "def fetch_arxiv_id_7(title):\n",
    "    title = f'ti:{cite['title']}'\n",
    "    client = arxiv.Client(num_retries=5)\n",
    "    search = arxiv.Search(query=title, max_results=3, sort_by=arxiv.SortCriterion.Relevance)\n",
    "    for result in client.results(search):\n",
    "        text = result.entry_id.split(\"/\")\n",
    "        return f\"{text[-2]}/{text[-1]}\"\n",
    "\n",
    "\n",
    "for f in tqdm.tqdm(glob.glob(\"crawled_papers/cs/*/citations-clean.jsonl\")):\n",
    "    for i, x in enumerate(yield_local(f)):\n",
    "        for m in x['citation']:\n",
    "            cite = x['citation'][m]\n",
    "            cite['paper'] = f.split(\"/\")[-2]\n",
    "            cite['sentence'] = i\n",
    "            cite['key'] = m\n",
    "            if \"source\" in cite: \n",
    "                if \"arXiv\" in cite['source']:\n",
    "                    if len(cite['volume']) == 7:\n",
    "                        if cite['volume'] in arxiv_set_7: cite['volume'] = arxiv_set_7[cite['volume']]\n",
    "                        else: \n",
    "                            arxiv_id = fetch_arxiv_id_7(cite['title'])\n",
    "                            if arxiv_id: cite['volume'] = arxiv_set_7[cite['volume']] = arxiv_id\n",
    "                            else: continue\n",
    "                    else:\n",
    "                        arxiv_set.add(cite['volume'])\n",
    "                paper_dataset[cite['source']].append(cite)\n",
    "            else:\n",
    "                if \"journal\" in cite:\n",
    "                    arxiv_id = arxiv_pattern.findall(cite['journal'])\n",
    "                    if arxiv_id:\n",
    "                        cite['source'] = \"inline arXiv\"\n",
    "                        cite['volume'] = arxiv_id[0]\n",
    "                        paper_dataset[cite['source']].append(cite)\n",
    "                        continue\n",
    "                    arxiv_id = re.findall(r\"(?<![0-9])[0-9]{7}(?![0-9])\", cite['journal'])\n",
    "                    if arxiv_id:\n",
    "                        cite['source'] = \"inline arXiv\"\n",
    "                        if arxiv_id[0] in arxiv_set_7: cite['volume'] = arxiv_set_7[arxiv_id[0]]\n",
    "                        else:\n",
    "                            arxiv_id = fetch_arxiv_id_7(cite['title'])\n",
    "                            if arxiv_id: cite['volume'] = arxiv_set_7[cite['volume']] = arxiv_id\n",
    "                            else: continue\n",
    "                        paper_dataset[cite['source']].append(cite)\n",
    "                        continue\n",
    "                if \"volume\" in cite:\n",
    "                    arxiv_id = arxiv_pattern.findall(cite['volume'])\n",
    "                    if arxiv_id:\n",
    "                        cite['source'] = \"inline arXiv\"\n",
    "                        cite['volume'] = arxiv_id[0]\n",
    "                        paper_dataset[cite['source']].append(cite)\n",
    "                        continue\n",
    "                    arxiv_id = re.findall(r\"(?<![0-9])[0-9]{7}(?![0-9])\", cite['volume'])\n",
    "                    if arxiv_id:\n",
    "                        cite['source'] = \"inline arXiv\"\n",
    "                        if arxiv_id[0] in arxiv_set_7: cite['volume'] = arxiv_set_7[arxiv_id[0]]\n",
    "                        else:\n",
    "                            arxiv_id = fetch_arxiv_id_7(cite['title'])\n",
    "                            if arxiv_id: cite['volume'] = arxiv_set_7[cite['volume']] = arxiv_id\n",
    "                            else: continue\n",
    "                        paper_dataset[cite['source']].append(cite)\n",
    "                        continue\n",
    "                paper_dataset[''].append(cite)\n",
    "print({k: len(v) for k, v in paper_dataset.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "64b21315-3e54-478d-a1fb-058bd9b74076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"2005.14165\" in arxiv_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1d226e44-318e-460d-9db7-14391436a5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inline arXiv': 49199, 'arXiv': 813, 's2': 4592, 'Network Error': 18, '': 10048, 'no title': 174}\n"
     ]
    }
   ],
   "source": [
    "paper_dataset_f = {\"inline arXiv\": [], \"arXiv\": [], \"s2\": [], \"Network Error\": [], \"\": [], \"no title\": []}\n",
    "title_to_url = {}\n",
    "for p in paper_dataset:\n",
    "    for x in paper_dataset[p]:\n",
    "        if 'title' not in x: \n",
    "            if 'info' not in x: \n",
    "                paper_dataset_f['no title'].append(x)\n",
    "                continue\n",
    "            else: x['title'] = x['info']\n",
    "        if x['title'] not in title_to_url:\n",
    "            if 'arXiv' in p:\n",
    "                title_to_url[x['title']] = x['volume']\n",
    "            elif p == 's2':\n",
    "                title_to_url[x['title']] = x['url']\n",
    "            elif 'url' in x:\n",
    "                x['source'] = 's2'\n",
    "                title_to_url[x['title']] = x['url']\n",
    "                paper_dataset_f['s2'].append(x)\n",
    "                continue\n",
    "            else:\n",
    "                if 'source' in x: del x['source']\n",
    "            paper_dataset_f[p].append(x)\n",
    "print({k: len(v) for k, v in paper_dataset_f.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "32a45ec7-57b6-41f7-be19-06ee30ad7984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'year': '2021', 'pages': '187--203', 'number': '1', 'volume': '43', 'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'author': 'Mar{\\\\i}n, Javier and Biswas, Aritro and Ofli, Ferda and Hynes, Nicholas and Salvador, Amaia and Aytar, Yusuf and Weber, Ingmar and Torralba, Antonio', 'title': 'Recipe1m+: A dataset for learning cross-modal embeddings for cooking recipes and food images', 'ENTRYTYPE': 'article', 'ID': 'marin2021recipe1m+', 'paper': '2501.01958', 'sentence': 9, 'key': 'marin2021recipe1m+'}\n"
     ]
    }
   ],
   "source": [
    "print(paper_dataset_f[''][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "304240df-9ae6-46f2-b3ea-d1cc70009577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inline arXiv': 49199, 'arXiv': 813, 's2': 4592, 'Network Error': 18, '': 10048, 'no title': 174}\n"
     ]
    }
   ],
   "source": [
    "# print_json([x for p in paper_dataset for x in paper_dataset[p]], \"crawled_papers/citations/all.jsonl\")\n",
    "print({k: len(v) for k, v in paper_dataset_f.items()})\n",
    "paper_dataset_f[''] += paper_dataset_f['Network Error']\n",
    "del paper_dataset_f['Network Error']\n",
    "for p in paper_dataset_f:\n",
    "    print_json(paper_dataset_f[p], f'crawled_papers/citations/{p.replace(\" \", \"_\") if p else \"null\"}.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae70f7d-1bd1-42eb-8217-c179130d82d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm, glob\n",
    "arxiv_set = {}\n",
    "for f in tqdm.tqdm(glob.glob(\"crawled_papers/cs/*/citations-clean.jsonl\")):\n",
    "    d = load_local(f)\n",
    "    for x in d:\n",
    "        for m in x['citation']:\n",
    "            cite = x['citation'][m]\n",
    "            if \"source\" in cite and \"arXiv\" in cite['source'] and len(cite['volume']) == 7:\n",
    "                if cite['volume'] in arxiv_set:\n",
    "                    cite['volume'] = arxiv_set[cite['volume']]\n",
    "                    continue\n",
    "                while True:\n",
    "                    try:\n",
    "                        title = f'ti:{cite['title']}'\n",
    "                        client = arxiv.Client(num_retries=retry)\n",
    "                        search = arxiv.Search(query=title, max_results=3, sort_by=arxiv.SortCriterion.Relevance)\n",
    "                        for r in client.results(search):\n",
    "                            text = result.entry_id.split(\"/\")\n",
    "                            cite['volume'] = arxiv_set[cite['volume']] = f\"{text[-2]}/{text[-1]}\"\n",
    "                            break\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        pass\n",
    "    print_json(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bc3da46c-5f2d-491a-a264-c1dd3ecbc574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inline_arXiv': 49199, 'arXiv': 916, 's2': 4661} 54773\n"
     ]
    }
   ],
   "source": [
    "sources = ['inline_arXiv', 'arXiv', 's2']\n",
    "paper_dataset = {k: load_local(f\"crawled_papers/citations/{k}.jsonl\") for k in sources}\n",
    "urls = {}\n",
    "for k in paper_dataset:\n",
    "    for x in paper_dataset[k]:\n",
    "        urls[x['title']] = x['url'] if k == \"s2\" else f\"https://arxiv.org/pdf/{x['volume']}\"\n",
    "print({k: len(v) for k, v in paper_dataset.items()}, len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4b5cb4b1-0f61-4aff-b52f-e35bab3af5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"crawled_papers/citations/urls.json\", \"w+\") as f: json.dump(urls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61e4601e-5017-49a0-8c35-818a566d66ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-21\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\"https://api.semanticscholar.org/datasets/v1/release/latest\").json()\n",
    "print(response['release_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bf1595ad-7fbd-41e4-80d9-909cd1152788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "s2orc = datasets.load_dataset(\"alienai/peS2O\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ccd24-cbde-4bc5-aa35-d5175860d14c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3840fa3-a987-4006-b43a-02c2fe0f06c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
